{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "from rl_utils.ppo_agent import PPOAgent\n",
    "from rl_utils.ppo_agent import EnhancedRewardCalculator, EnhancedStateHandler\n",
    "\n",
    "import rl_utils.abm3 as abm3\n",
    "import rl_utils.repositioning as repositioning\n",
    "\n",
    "from rl_utils.PPOSystemMetrics import SystemMonitor\n",
    "from rl_utils.PPOAgentVisualizer import RLAgentVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\"Data/TrainData.csv\")\n",
    "rejection_model = joblib.load('Data/rejection_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train on the day one week before\n",
    "initial_timestart = 1666591200-86400*7\n",
    "start_time = pd.to_datetime(initial_timestart, unit='s') + pd.Timedelta(hours=8)\n",
    "\n",
    "# End of three hours period\n",
    "end_time = start_time + pd.Timedelta(hours=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['platform_order_date'] = pd.to_datetime(orders['platform_order_date'])\n",
    "orders = orders[\n",
    "    (orders['platform_order_date'] > start_time) &\n",
    "    (orders['platform_order_date'] < end_time)\n",
    "]\n",
    "\n",
    "# Create bins for \"demand prediction\", here we take the actual demand for repositioning\n",
    "orders['time_bin'] = orders['platform_order_date'].dt.floor('15min')\n",
    "\n",
    "# Numbers of orders per bin\n",
    "actual_demand = (\n",
    "    orders.groupby(['time_bin', 'hex_id'])\n",
    "    .size()\n",
    "    .reset_index(name='actual_order_count')\n",
    ")\n",
    "\n",
    "# The same format as our predicted values for the ABM.ipynb\n",
    "wide_actual = actual_demand.pivot(\n",
    "    index='time_bin',\n",
    "    columns='hex_id',\n",
    "    values='actual_order_count'\n",
    ").fillna(0)\n",
    "pre_binned_demand = wide_actual.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_handler = EnhancedStateHandler(grid= None) #Activate state handler\n",
    "reward_calculator = EnhancedRewardCalculator(base_reward=1.0) #Activate reward handler with base values\n",
    "state_dim = 4  # [norm_distance, courier_availability]\n",
    "action_dim = 2 # [0: direct, 1: split]\n",
    "agent = PPOAgent(state_dim=state_dim, action_dim=action_dim)#\n",
    "monitor = SystemMonitor()\n",
    "visualizer = RLAgentVisualizer(agent, state_handler, grid=None, monitor=monitor)\n",
    "\n",
    "NUM_EPISODES = 200\n",
    "TRAIN_INTERVAL = 512\n",
    "WARMUP_DURATION_SECONDS = 30 * 60\n",
    "\n",
    "\n",
    "constants = {\n",
    "    'initial_timestart': 1666591200-86400*7,\n",
    "    'SPEED_HEX_PER_STEP': 8,\n",
    "    'simulation_duration_hours': 3,\n",
    "    'steps': 30,\n",
    "    'repositioning_interval': 15 * 60,\n",
    "    'MAX_ACCEPTABLE_DELAY_SECONDS': 5 * 60,\n",
    "    'MAX_QUEUE_ATTEMPTS': 20,\n",
    "    'pre_binned_demand': pre_binned_demand,\n",
    "    'MACRO_RESOLUTION': 8,\n",
    "    'WORK_RESOLUTION': 13\n",
    "}\n",
    "sim_data_master = orders.copy() \n",
    "timestart = constants['initial_timestart']\n",
    "last_order_time = sim_data_master['platform_order_time'].max()\n",
    "TIME_BUFFER_SECONDS = 15 * 60\n",
    "timeend = last_order_time + TIME_BUFFER_SECONDS\n",
    "\n",
    "\n",
    "rejection_model = joblib.load('Data/rejection_model.joblib')\n",
    "\n",
    "episode_metrics = defaultdict(list)\n",
    "\n",
    "base_coverage = 0.8\n",
    "variation_range = 0.3\n",
    "target_utilization = 0.25\n",
    "\n",
    "learning_start_time = timestart + WARMUP_DURATION_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starte Episode 1/300 | Kuriere: 1142 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# assign reposition tasks\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dynamic_demand:\n\u001b[1;32m---> 54\u001b[0m         \u001b[43mrepositioning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_repositioning_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcouriers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_demand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPEED_HEX_PER_STEP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMACRO_RESOLUTION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWORK_RESOLUTION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_time \u001b[38;5;241m<\u001b[39m timeend:\n\u001b[0;32m     60\u001b[0m     \n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# 1. SYSTEM-UPDATE: Bewege Kuriere und verarbeite abgeschlossene Aufträge\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     couriers, delivered_orders \u001b[38;5;241m=\u001b[39m abm3\u001b[38;5;241m.\u001b[39mupdate_couriers_and_system(\n\u001b[0;32m     63\u001b[0m         current_time, constants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m], couriers, delivered_orders, constants, pre_binned_demand, order_queue\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\freud\\OneDrive - Universität Münster\\Master Thesis\\Python Code Thesis\\rl_utils\\repositioning.py:170\u001b[0m, in \u001b[0;36mrun_repositioning_strategy\u001b[1;34m(couriers, demand_map, timestart, order_queue, speed_hex_per_step, steps, macro_resolution, work_resolution)\u001b[0m\n\u001b[0;32m    167\u001b[0m surplus_zones, deficit_zones \u001b[38;5;241m=\u001b[39m identify_courier_demand_zones(surplus_deficit_map)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m surplus_zones \u001b[38;5;129;01mand\u001b[39;00m deficit_zones:\n\u001b[1;32m--> 170\u001b[0m     \u001b[43mexecute_repositioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43midle_couriers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurplus_zones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeficit_zones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed_hex_per_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmacro_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwork_resolution\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\freud\\OneDrive - Universität Münster\\Master Thesis\\Python Code Thesis\\rl_utils\\repositioning.py:90\u001b[0m, in \u001b[0;36mexecute_repositioning\u001b[1;34m(idle_couriers, surplus_zones, deficit_zones, timestart, order_queue, speed_hex_per_step, steps, macro_resolution, work_resolution)\u001b[0m\n\u001b[0;32m     88\u001b[0m max_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s_zone \u001b[38;5;129;01min\u001b[39;00m surplus_zones: \u001b[38;5;66;03m#check if available couriers\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     has_available_courier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mh3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_to_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmacro_resolution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms_zone\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mavailable_couriers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_available_courier: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m#high surplus and low distance = best\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\freud\\OneDrive - Universität Münster\\Master Thesis\\Python Code Thesis\\rl_utils\\repositioning.py:90\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     88\u001b[0m max_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s_zone \u001b[38;5;129;01min\u001b[39;00m surplus_zones: \u001b[38;5;66;03m#check if available couriers\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     has_available_courier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[43mh3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_to_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmacro_resolution\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m s_zone[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m available_couriers)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_available_courier: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m#high surplus and low distance = best\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\freud\\anaconda3\\envs\\tf310\\lib\\site-packages\\h3\\api\\basic_str\\__init__.py:225\u001b[0m, in \u001b[0;36mcell_to_parent\u001b[1;34m(h, res)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcell_to_parent\u001b[39m(h, res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    Get the parent of a cell.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    H3Cell\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43m_in_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     p \u001b[38;5;241m=\u001b[39m _cy\u001b[38;5;241m.\u001b[39mcell_to_parent(h, res)\n\u001b[0;32m    227\u001b[0m     p \u001b[38;5;241m=\u001b[39m _out_scalar(p)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# HAUPT-TRAININGSSCHLEIFE\n",
    "# ===================================================================\n",
    "experienced_states_for_plotting = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    current_time = timestart\n",
    "    order_queue = []\n",
    "    availability_history = []\n",
    "\n",
    "    avg_order_duration = 1800\n",
    "    orders_in_window = len(sim_data_master) / constants.get('simulation_duration_hours', 3)\n",
    "    required_courier_hours = (orders_in_window * avg_order_duration) / (3600*3)\n",
    "\n",
    "    coverage_adjustment = np.random.uniform(-variation_range, variation_range)\n",
    "    adjusted_coverage = max(0.3, min(0.9, base_coverage + coverage_adjustment))\n",
    "    num_couriers_for_episode = max(int(required_courier_hours / target_utilization * adjusted_coverage), 5)\n",
    "    \n",
    "\n",
    "    couriers = abm3.initiate_couriers(\n",
    "            total_couriers_to_create=num_couriers_for_episode,\n",
    "            data_source_df=sim_data_master\n",
    "        )    \n",
    "\n",
    "    episode_reward_sum = 0\n",
    "    decision_counter = 0\n",
    "    delivered_orders = set()\n",
    "    current_time = timestart\n",
    "    \n",
    "    \n",
    "    print(f\"--- Starte Episode {episode + 1}/{NUM_EPISODES} | Kuriere: {num_couriers_for_episode} ---\")\n",
    "\n",
    "    \n",
    "            #As we think that the fleet would not start at point zero\n",
    "    warmup_seconds = 15 * 60 \n",
    "    warmup_start_time = constants['initial_timestart']  - warmup_seconds\n",
    "    \n",
    "    for t in range(warmup_start_time, constants['initial_timestart'] , constants['steps'] ):\n",
    "        # Move couriers in repositioning task\n",
    "        couriers, _, delivered_order_ids = abm3.move_couriers_new(couriers, \n",
    "        timestart, \n",
    "        (0, 0, 0, 0), # Metriken werden jetzt extern getrackt\n",
    "        delivered_orders,\n",
    "        constants['SPEED_HEX_PER_STEP'], \n",
    "        constants['steps']\n",
    "    )\n",
    "\n",
    "            # Use the pre-binned forecast for the first time-slot as the warm-up target.\n",
    "        first_bin_key = pd.to_datetime(constants['initial_timestart'] , unit='s').floor('15min') + pd.Timedelta(hours=8)\n",
    "        dynamic_demand = pre_binned_demand.get(first_bin_key, {})\n",
    "\n",
    "        # assign reposition tasks\n",
    "        if dynamic_demand:\n",
    "            repositioning.run_repositioning_strategy(couriers, dynamic_demand, t, [], \n",
    "                constants['SPEED_HEX_PER_STEP'], constants['steps'], \n",
    "                constants['MACRO_RESOLUTION'], constants['WORK_RESOLUTION']\n",
    "            )\n",
    "\n",
    "    while current_time < timeend:\n",
    "        \n",
    "        # 1. SYSTEM-UPDATE: Bewege Kuriere und verarbeite abgeschlossene Aufträge\n",
    "        couriers, delivered_orders = abm3.update_couriers_and_system(\n",
    "            current_time, constants['steps'], couriers, delivered_orders, constants, pre_binned_demand, order_queue\n",
    "        )\n",
    "\n",
    "        if current_time >= learning_start_time:\n",
    "            active_couriers = [c for c in couriers if c.state != 'INACTIVE']\n",
    "            if active_couriers:\n",
    "                idle_couriers = [c for c in active_couriers if c.state == 'IDLE']\n",
    "                current_availability = len(idle_couriers) / len(active_couriers)\n",
    "                availability_history.append(current_availability)\n",
    "                # 2. DATEN-ABRUF: Hole neue Aufträge für diesen Zeitschritt\n",
    "\n",
    "\n",
    "        new_orders = abm3.get_new_orders(current_time, constants['steps'], sim_data_master)\n",
    "\n",
    "    # 3. NEUE WARTESCHLANGEN-LOGIK: Alle anstehenden Aufträge vorbereiten\n",
    "        all_pending_orders = order_queue + [(order, 0) for order in new_orders]\n",
    "        next_order_queue = [] # Bereitet die Queue für den nächsten Zeitschritt vor\n",
    "        global_features = state_handler.get_global_state_features(couriers, all_pending_orders)\n",
    "\n",
    "        # 4. ENTSCHEIDUNGSFINDUNG: Verarbeite alle anstehenden Aufträge\n",
    "        for order, attempts in all_pending_orders:\n",
    "            \n",
    "            success = False # Wird für die Entscheidung benötigt, ob der Auftrag in der Queue bleibt\n",
    "\n",
    "            # 4a. FALLBACK-LOGIK: Wenn ein Auftrag zu lange wartet\n",
    "            if attempts > constants.get('MAX_QUEUE_ATTEMPTS', 5):\n",
    "                # Erzwinge eine Standard-Zuweisung ohne Agenten-Entscheidung\n",
    "                # WICHTIG: handle_standard_assignment muss auch `success` zurückgeben\n",
    "                processed_ids_set = set() # Platzhalter\n",
    "                success, _ = abm3.handle_standard_assignment(\n",
    "                    order, attempts, couriers, current_time, constants, rejection_model, processed_ids_set\n",
    "                )\n",
    "                # Hier findet kein Training des Agenten statt, da es eine Regel-basierte Entscheidung war.\n",
    "\n",
    "            # 4b. AGENTEN-LOGIK: Standard-Verarbeitung durch den PPO-Agenten\n",
    "            else:\n",
    "                #state_features = state_handler.get_state_features(order, couriers, all_pending_orders) # Nutze die komplette Liste für den State\n",
    "                \n",
    "\n",
    "                order_distance_feature = state_handler.get_order_specific_feature(order)\n",
    "                state_features = np.concatenate(([order_distance_feature], global_features))\n",
    "                experienced_states_for_plotting.append(state_features)\n",
    "\n",
    "                # --- WARM-UP PHASE ---\n",
    "                if current_time < learning_start_time:\n",
    "                    action = 0 # Standard-Aktion\n",
    "                    _, _, success = abm3.execute_decision_for_order( # WICHTIG: success wird hier ausgelesen\n",
    "                        order, action, couriers, current_time, constants, \n",
    "                        reward_calculator, state_handler, rejection_model, state_features\n",
    "                    )\n",
    "                # --- LERN-PHASE ---\n",
    "                else:\n",
    "                    action, log_prob, action_probs_tensor = agent.select_action(state_features)\n",
    "                    action_probs = action_probs_tensor.numpy()[0] # Konvertiere zu NumPy-Array                                             \n",
    "                    # Führe Aktion aus und erhalte Ergebnisse (inkl. `success`!)\n",
    "                    reward, done, success = abm3.execute_decision_for_order(\n",
    "                        order, action, couriers, current_time, constants, \n",
    "                        reward_calculator, state_handler, rejection_model, state_features=state_features\n",
    "                    )\n",
    "                    visualizer.record_decision(state_features, action, reward, action_probs)\n",
    "                    \n",
    "                    # Speichere Erfahrung und trainiere\n",
    "                    agent.store_transition(state_features, action, reward, log_prob.numpy(), done)\n",
    "                    episode_reward_sum += reward\n",
    "                    decision_counter += 1\n",
    "                    if decision_counter % TRAIN_INTERVAL == 0 and decision_counter > 0:\n",
    "                        actor_loss, critic_loss, entropy = agent.train()\n",
    "                        episode_metrics['actor_loss'].append(actor_loss)\n",
    "                        episode_metrics['critic_loss'].append(critic_loss)\n",
    "                        episode_metrics['entropy'].append(entropy)\n",
    "\n",
    "            # 5. NEUE WARTESCHLANGEN-LOGIK: Queue für nächsten Schritt aktualisieren\n",
    "            if not success:\n",
    "                next_order_queue.append((order, attempts + 1))\n",
    "\n",
    "        # Die aktualisierte Warteschlange für den nächsten Zeitschritt übernehmen\n",
    "        order_queue = next_order_queue\n",
    "                \n",
    "        # 6. Zeit fortschreiten lassen\n",
    "        current_time += constants['steps']\n",
    "\n",
    "    # --- ENDE DER EPISODE ---\n",
    "    # Trainiere mit den verbleibenden Daten im Speicher\n",
    "    if len(agent.states) > 0:\n",
    "        print(\"  -> Finales Training am Ende der Episode...\")\n",
    "        agent.train()\n",
    "    \n",
    "    del couriers\n",
    "    del availability_history\n",
    "    gc.collect()\n",
    "    \n",
    "    avg_reward_per_decision = episode_reward_sum / decision_counter if decision_counter > 0 else 0\n",
    "    episode_metrics['rewards'].append(avg_reward_per_decision)\n",
    "    # Berechne die Durchschnittswerte der neuen Metriken für die Ausgabe\n",
    "    avg_actor_loss = np.mean(episode_metrics['actor_loss'][-decision_counter:]) if decision_counter > 0 else 0\n",
    "    avg_critic_loss = np.mean(episode_metrics['critic_loss'][-decision_counter:]) if decision_counter > 0 else 0\n",
    "    avg_entropy = np.mean(episode_metrics['entropy'][-decision_counter:]) if decision_counter > 0 else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1} beendet. Avg Reward: {avg_reward_per_decision:.3f} | \"\n",
    "        f\"Actor Loss: {avg_actor_loss:.3f} | Critic Loss: {avg_critic_loss:.3f} | \"\n",
    "        f\"Entropy: {avg_entropy:.3f}\\\\n\")\n",
    "    \n",
    "    plot_dir = \"newplots\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    if (episode + 1) % 5 == 1:\n",
    "        visualizer.clear_history()\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(\"\\n--- Evaluierungs-Checkpoint ---\") \n",
    "\n",
    "        if experienced_states_for_plotting: # Only plot if data has been collected\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            fig.suptitle('Learned Policy with Experienced States Overlay', fontsize=16)\n",
    "\n",
    "            # Step A: Draw the policy heatmap exactly as before\n",
    "            im = visualizer.visualize_unified_policy(ax)\n",
    "            fig.colorbar(im, ax=ax, label=\"Probability of 'Split' (Action 1)\")\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            policy_plot_filename = os.path.join(plot_dir, f\"policy_episode_{episode + 1}_experience.svg\")\n",
    "            fig.savefig(policy_plot_filename, format='svg', bbox_inches='tight')\n",
    "            plt.show(fig)\n",
    "        \n",
    "        print(\"  -> Erstelle Plot zur Entscheidungsverteilung...\")\n",
    "        fig_dist = visualizer.visualize_decision_distribution()\n",
    "        if fig_dist:\n",
    "            dist_plot_filename = os.path.join(plot_dir, f\"decision_dist_episode_{episode + 1}.svg\")\n",
    "            fig_dist.savefig(dist_plot_filename, format='svg', bbox_inches='tight' )\n",
    "            plt.show(fig_dist)\n",
    "\n",
    "        print(\" -> Erstelle Plot zur Belohnung vs. Zuversicht...\")\n",
    "        fig_confidence = visualizer.visualize_reward_confidence()\n",
    "        if fig_confidence:\n",
    "            confidence_plot_filename = os.path.join(plot_dir, f\"confidence_episode_{episode + 1}.svg\")\n",
    "            fig_confidence.savefig(confidence_plot_filename, format='svg', bbox_inches='tight' )\n",
    "            plt.show()\n",
    "\n",
    "        print(\" -> Erstelle Plot zur Aktions-Wahrscheinlichkeit...\")\n",
    "        fig_probs = visualizer.visualize_action_probabilities()\n",
    "        if fig_probs:\n",
    "            probs_plot_filename = os.path.join(plot_dir, f\"probability_episode_{episode + 1}.svg\")\n",
    "            fig_probs.savefig(probs_plot_filename, format='svg', bbox_inches='tight' )\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        # WICHTIG: Leere die History, damit die nächste Analyse frisch startet\n",
    "        visualizer.clear_history()\n",
    "        \n",
    "        # Der Code für den Reward-Plot bleibt unverändert\n",
    "        rewards = episode_metrics['rewards']\n",
    "\n",
    "        # Erstelle ein Pandas DataFrame für einfache Berechnungen\n",
    "        df = pd.DataFrame({'rewards': rewards})\n",
    "\n",
    "        # Berechne den gleitenden Durchschnitt\n",
    "        window_size = 30\n",
    "        df['moving_average'] = df['rewards'].rolling(window=window_size).mean()\n",
    "\n",
    "        # Erstelle den Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index, df['rewards'], alpha=0.3, label='Belohnung pro Episode')\n",
    "        plt.plot(df.index, df['moving_average'], color='red', linewidth=2, label=f'Gleitender Durchschnitt ({window_size} Episoden)')\n",
    "\n",
    "        plt.title('Trainingsfortschritt: Belohnung über Episoden', fontsize=16)\n",
    "        plt.xlabel('Episode', fontsize=12)\n",
    "        plt.ylabel('Avg Reward per Decision', fontsize=12) # Angepasst für Klarheit\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        reward_plot_filename = os.path.join(plot_dir, f\"reward_episode_{episode + 1}.svg\")\n",
    "        plt.savefig(reward_plot_filename, format='svg', bbox_inches='tight')   \n",
    "        plt.show()\n",
    "    \n",
    "    if (episode + 1) % 1 == 0:\n",
    "        agent.save_models(f\"final_ppo_agent_{episode + 1}\") # Ruft die neue Speicherfunktion auf\n",
    "\n",
    "print(\"Training abgeschlossen und finale Modelle gespeichert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
