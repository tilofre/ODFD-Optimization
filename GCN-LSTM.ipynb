{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7747231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Reshape\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the already prepared data\n",
    "aggLSTM = pd.read_parquet(\"Data/agg_table_new_zones8Bot.parquet\")\n",
    "#Create a pivot table\n",
    "pivot_df = aggLSTM.pivot(index='time_bin', columns='h3_index', values='order_count').fillna(0)\n",
    "#Create adjacency matrix on neighbours\n",
    "adj_matrix = pd.read_parquet(\"Data/neighbours_zones_8.parquet\")\n",
    "adj_matrix.index = adj_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b66188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling for normalisation\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sliding window for four steps\n",
    "def create_dataset_4d(data, time_steps=4):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps, :].reshape((time_steps, data.shape[1], 1)))\n",
    "        y.append(data[i+time_steps, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LEN = 4\n",
    "X, y = create_dataset_4d(scaled_data, time_steps=SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One day test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.125, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ one day validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[..., np.newaxis]  # shape for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a956482",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj_matrix\n",
    "adj = adj + np.eye(adj.shape[0])  # Self-loop added\n",
    "\n",
    "def normalize_adj(adj): #normalize adjacency feature\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(adj.sum(axis=1)))\n",
    "    return D_inv_sqrt @ adj @ D_inv_sqrt\n",
    "\n",
    "adj_normalized = normalize_adj(adj).astype(np.float32)\n",
    "adj_tensor = tf.constant(adj_matrix, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52738139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(tf.keras.layers.Layer):#Create GCN class for LSTM-GCN combination\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "    def call(self, inputs, adj):\n",
    "        x = tf.matmul(adj, inputs)  # (batch, nodes, features)\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeDistributedGCN(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, adj_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.gcn = GraphConvolution(output_dim)\n",
    "\n",
    "    def call(self, inputs):  # inputs: (batch, time, nodes, features)\n",
    "        outputs = []\n",
    "        for t in range(inputs.shape[1]):\n",
    "            x_t = inputs[:, t, :, :]  # (batch, nodes, features)\n",
    "            out_t = self.gcn(x_t, self.adj_matrix)  # (batch, nodes, output_dim)\n",
    "            outputs.append(out_t)\n",
    "        return tf.stack(outputs, axis=1)  # (batch, time, nodes, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining gcn with lstm\n",
    "def build_gcn_lstm_model(num_nodes, seq_len, feature_dim, adj_matrix, gcn_units, lstm_units):\n",
    "    X_input = Input(shape=(seq_len, num_nodes, feature_dim))\n",
    "    \n",
    "    gcn_td = TimeDistributedGCN(gcn_units, adj_matrix)\n",
    "    gcn_output = gcn_td(X_input)  # (batch, time, nodes, gcn_units)\n",
    "    \n",
    "    reshaped = Reshape((seq_len, num_nodes * gcn_units))(gcn_output)\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=False)(reshaped)\n",
    "    \n",
    "    output = Dense(num_nodes)(lstm_out)\n",
    "\n",
    "    return Model(inputs=X_input, outputs=output)\n",
    "\n",
    "\n",
    "#Use model for training\n",
    "model = build_gcn_lstm_model(\n",
    "    num_nodes=X.shape[2],\n",
    "    seq_len=X.shape[1],\n",
    "    feature_dim=1,\n",
    "    adj_matrix=adj_tensor,\n",
    "    gcn_units=16,\n",
    "    lstm_units=64\n",
    ")\n",
    "def log_cosh_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.log(tf.cosh(y_pred - y_true)))\n",
    "\n",
    "model.compile(optimizer='adam', loss=log_cosh_loss)#Calculate training losses\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "print(\"Begin Training...\")\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction and rescale the values\n",
    "predictions = model.predict(X_test)\n",
    "predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "y_test_rescaled = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8143340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all values for metrics calculation and visualizaion\n",
    "y_tests_all = np.concatenate(y_test_rescaled)\n",
    "y_preds_all = np.concatenate(predictions_rescaled)\n",
    "n = len(y_tests_all)\n",
    "p = X.shape[1]\n",
    "\n",
    "#all necessary metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_tests_all, y_preds_all))\n",
    "mae = mean_absolute_error(y_tests_all, y_preds_all)\n",
    "r2 = r2_score(y_tests_all, y_preds_all)\n",
    "nrmse = rmse / np.mean(np.abs(y_tests_all)) * 100\n",
    "nonzero_indices = y_tests_all != 0\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "#To not divide by zero\n",
    "if np.any(nonzero_indices):\n",
    "    mape = np.mean(np.abs((y_tests_all[nonzero_indices] - y_preds_all[nonzero_indices]) / y_tests_all[nonzero_indices])) * 100\n",
    "else:\n",
    "    mape = np.nan\n",
    "\n",
    "print(\"Evaluation über alle Areas hinweg:\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"NRMSE: {nrmse:.2f}%\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "print(f\"Adjusted R²: {adj_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_area = \"88329b5aa1fffff\"  # The area to compare\n",
    "example_index = pivot_df.columns.get_loc(example_area)\n",
    "np.save('prognose_GCN_LSTM.npy', predictions_rescaled[:, example_index]) #Save output numpy file\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test_rescaled[:, example_index], label='Actual')\n",
    "plt.plot(predictions_rescaled[:, example_index], label='Prediction')\n",
    "plt.title(f'LSTM Prediction {example_area}')\n",
    "plt.xlabel('Test-Time bins')\n",
    "plt.ylabel('Orders')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for all comnined predictions total actual vs. total predicted\n",
    "total_actual = y_test_rescaled.sum(axis=1)\n",
    "total_pred = predictions_rescaled.sum(axis=1)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(total_actual, total_pred))\n",
    "mae = mean_absolute_error(total_actual, total_pred)\n",
    "r2 = r2_score(total_actual, total_pred)\n",
    "mape = np.mean(np.abs((total_actual - total_pred) / np.maximum(total_actual, 1e-5))) * 100\n",
    "\n",
    "print(\"\\n--- Evaluation ---\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"R²:   {r2:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af44b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulated demand over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_actual, label=\"Actual Total Demand\")\n",
    "plt.plot(total_pred, label=\"Predicted Total Demand\")\n",
    "plt.title(\"Demand\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Orders\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
