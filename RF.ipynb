{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggRF = pd.read_parquet(\"Data/agg_table_new_zones8Bot.parquet\")\n",
    "pivot_df = aggRF.pivot(index='time_bin', columns='h3_index', values='order_count').fillna(0)\n",
    "adj_matrix = pd.read_parquet(\"Data/neighbours_zones_8.parquet\")\n",
    "adj_matrix.index = adj_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8932c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix.index = adj_matrix.index.astype(str)\n",
    "adj_matrix.columns = adj_matrix.columns.astype(str)\n",
    "\n",
    "original_area_columns = pivot_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211552ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine matrix with pivot\n",
    "common_indices = list(set(original_area_columns) & set(adj_matrix.index) & set(adj_matrix.columns))\n",
    "neighbor_df = adj_matrix.loc[common_indices, common_indices]\n",
    "\n",
    "\n",
    "# Create a dictionary map based on neighbours\n",
    "h3_indices_in_pivot = original_area_columns # Use the order\n",
    "neighbor_map = {h3_index: [] for h3_index in h3_indices_in_pivot}\n",
    "\n",
    "neighbor_df_indices = neighbor_df.index.tolist()\n",
    "\n",
    "# Walk through of each area\n",
    "for center_h3 in h3_indices_in_pivot:\n",
    "    if center_h3 in neighbor_df_indices:\n",
    "        # Find neighbours\n",
    "        neighbors = neighbor_df.columns[neighbor_df.loc[center_h3] == 1].tolist()\n",
    "        # Add neighbours \n",
    "        valid_neighbors = [nb for nb in neighbors if nb in h3_indices_in_pivot and nb != center_h3]\n",
    "        neighbor_map[center_h3] = valid_neighbors\n",
    "\n",
    "\n",
    "# Cumulate the orders of neighbours\n",
    "pivot_df_with_neighbors = pivot_df.copy()\n",
    "neighbor_sum_cols = []\n",
    "\n",
    "for h3_index in original_area_columns:\n",
    "    neighbor_feature_name = f\"{h3_index}_neighbor_sum\"\n",
    "    neighbor_sum_cols.append(neighbor_feature_name)\n",
    "    # Use neighbours of first mapping\n",
    "    valid_neighbors = [neighbor for neighbor in neighbor_map.get(h3_index, []) if neighbor in pivot_df.columns]\n",
    "\n",
    "    if valid_neighbors:\n",
    "        pivot_df_with_neighbors[neighbor_feature_name] = pivot_df[valid_neighbors].sum(axis=1)\n",
    "    else:\n",
    "        pivot_df_with_neighbors[neighbor_feature_name] = 0.0\n",
    "\n",
    "\n",
    "pivot_df = pivot_df_with_neighbors #The new data frame for going further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time features\n",
    "pivot_df = pivot_df.reset_index()\n",
    "pivot_df['hour'] = pivot_df['time_bin'].dt.hour\n",
    "pivot_df['dayofweek'] = pivot_df['time_bin'].dt.dayofweek \n",
    "\n",
    "# Cylcical features (like a clock)\n",
    "pivot_df['hour_sin'] = np.sin(2 * np.pi * pivot_df['hour'] / 24)\n",
    "pivot_df['hour_cos'] = np.cos(2 * np.pi * pivot_df['hour'] / 24)\n",
    "pivot_df['day_sin'] = np.sin(2 * np.pi * pivot_df['dayofweek'] / 7)\n",
    "pivot_df['day_cos'] = np.cos(2 * np.pi * pivot_df['dayofweek'] / 7)\n",
    "time_bins_original = pivot_df['time_bin'].copy()\n",
    "\n",
    "# Remove time features\n",
    "pivot_df = pivot_df.drop(columns=['time_bin', 'hour', 'dayofweek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_columns = original_area_columns\n",
    "time_feature_cols = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
    "n_areas = len(area_columns)\n",
    "ordered_cols = area_columns + neighbor_sum_cols + time_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7212e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_for_model = pivot_df[ordered_cols]\n",
    "data = pivot_df_for_model.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9253fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, time_steps=4, n_target_features=None, n_neighbor_features=None, n_time_features=None):\n",
    "    X, y = [], []\n",
    "    target_end_idx = n_target_features\n",
    "    neighbor_end_idx = n_target_features + n_neighbor_features\n",
    "    time_start_idx = neighbor_end_idx\n",
    "    for i in range(len(data) - time_steps):\n",
    "        # area counts lagged\n",
    "        lagged_targets_part = data[i : i + time_steps, :target_end_idx].flatten()\n",
    "        # lagged neighbours\n",
    "        lagged_neighbors_part = data[i : i + time_steps, target_end_idx : neighbor_end_idx].flatten()\n",
    "        # time features\n",
    "        current_time_features = data[i + time_steps, time_start_idx :]\n",
    "        # combination of targets\n",
    "        X.append(np.concatenate([lagged_targets_part, lagged_neighbors_part, current_time_features]))\n",
    "        # Objective\n",
    "        y.append(data[i + time_steps, :target_end_idx])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 4\n",
    "n_target_features = n_areas\n",
    "n_neighbor_features = len(neighbor_sum_cols) \n",
    "n_time_features = len(time_feature_cols)\n",
    "\n",
    "X, y = create_dataset(data, #use features\n",
    "                      time_steps=time_steps,\n",
    "                      n_target_features=n_target_features,\n",
    "                      n_neighbor_features=n_neighbor_features,\n",
    "                      n_time_features=n_time_features)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.125, shuffle=False # Last day of set\n",
    ")\n",
    "#Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "predictions = np.zeros_like(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_area_for_tuning = \"88329b5aa1fffff\" #our zone\n",
    "try:\n",
    "    example_index = area_columns.index(example_area_for_tuning)\n",
    "    print(f\"Optimiere Hyperparameter für Beispiel-Zone: {example_area_for_tuning} (Index {example_index})...\")\n",
    "\n",
    "    # Definition for grid search finding best params\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 400], # Reduziert für schnellere Suche\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 7],\n",
    "        'min_samples_leaf': [1, 2,3]\n",
    "    }\n",
    "\n",
    "    # Erstelle das GridSearchCV-Objekt\n",
    "    rf_for_tuning = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search = GridSearchCV(estimator=rf_for_tuning,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=3, #  3 fold cross validation\n",
    "                               n_jobs=-1, # Parralelisation \n",
    "                               verbose=1,\n",
    "                               scoring='neg_mean_squared_error') # optimisation loss\n",
    "\n",
    "    # Just for one example zone, it is our best one\n",
    "    grid_search.fit(X_train, y_train[:, example_index])\n",
    "\n",
    "    # Found best params\n",
    "    best_params = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model with the best parameters you have found\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "num_areas_to_train = y_train.shape[1]\n",
    "for i in range(num_areas_to_train):\n",
    "    # Create for each zone a model\n",
    "    rf_model = RandomForestRegressor(random_state=42, n_jobs=-1, **best_params) # n_jobs=-1 for parralelisation\n",
    "\n",
    "    # train model for zone i\n",
    "    rf_model.fit(X_train, y_train[:, i])\n",
    "\n",
    "    # predict values\n",
    "    pred = rf_model.predict(X_test)\n",
    "\n",
    "    # save predictions and save\n",
    "    predictions[:, i] = pred\n",
    "    models.append(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of results\n",
    "y_tests_all = y_test.flatten()\n",
    "y_preds_all = predictions.flatten()\n",
    "\n",
    "# Evaluationmetrics\n",
    "n = len(y_tests_all) # Number of preds\n",
    "p = X_train.shape[1] # Number of features\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_tests_all, y_preds_all))\n",
    "mae = mean_absolute_error(y_tests_all, y_preds_all)\n",
    "r2 = r2_score(y_tests_all, y_preds_all)\n",
    "mean_abs_actual = np.mean(np.abs(y_tests_all))\n",
    "if mean_abs_actual == 0:\n",
    "    nrmse = np.nan\n",
    "else:\n",
    "    nrmse = rmse / mean_abs_actual * 100\n",
    "\n",
    "# MAPE Calculation\n",
    "nonzero_indices = y_tests_all != 0\n",
    "if np.any(nonzero_indices):\n",
    "    mape = np.mean(np.abs((y_tests_all[nonzero_indices] - y_preds_all[nonzero_indices]) / y_tests_all[nonzero_indices])) * 100\n",
    "else:\n",
    "    mape = np.nan\n",
    "\n",
    "# Adjusted R² Calculation\n",
    "if n - p - 1 <= 0: \n",
    "    adj_r2 = np.nan\n",
    "else:\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"NRMSE: {nrmse:.2f}%\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "print(f\"Adjusted R²: {adj_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_area_plot = \"88329b5aa1fffff\" # Beispielzone für den Plot\n",
    "example_index_plot = area_columns.index(example_area_plot)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test[:, example_index_plot], label='Actual')\n",
    "plt.plot(predictions[:, example_index_plot], label='Prediction')\n",
    "plt.title(f'Random Forest Prediction (mit Nachbarn) für {example_area_plot}')\n",
    "plt.xlabel('Test-Zeitintervalle')\n",
    "plt.ylabel('Bestellungen')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_actual_per_step = np.sum(y_test, axis=1)\n",
    "total_predicted_per_step = np.sum(predictions, axis=1)\n",
    "\n",
    "rmse_total = np.sqrt(mean_squared_error(total_actual_per_step, total_predicted_per_step))\n",
    "mae_total = mean_absolute_error(total_actual_per_step, total_predicted_per_step)\n",
    "r2_total = r2_score(total_actual_per_step, total_predicted_per_step)\n",
    "mask_total = total_actual_per_step != 0\n",
    "if np.any(mask_total):\n",
    "    mape_total = np.mean(np.abs((total_actual_per_step[mask_total] - total_predicted_per_step[mask_total]) / total_actual_per_step[mask_total])) * 100\n",
    "else:\n",
    "    mape_total = np.nan\n",
    "\n",
    "print(f\"\\n--- Metriken für Summe über alle Areas pro Zeitintervall (Random Forest mit Nachbarn) ---\")\n",
    "print(f\"RMSE (Summe): {rmse_total:.2f}\")\n",
    "print(f\"MAE  (Summe): {mae_total:.2f}\")\n",
    "print(f\"R²   (Summe): {r2_total:.2f}\")\n",
    "print(f\"MAPE (Summe): {mape_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ce912",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(total_actual_per_step, label='Actual (Summe)')\n",
    "plt.plot(total_predicted_per_step, label='Prediction (Summe)')\n",
    "plt.title('Random Forest Prediction (Summe über alle Zonen pro Zeitintervall, mit Nachbarn)')\n",
    "plt.xlabel('Test-Zeitintervalle')\n",
    "plt.ylabel('Gesamtanzahl Bestellungen')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09dccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_area = \"88329b5aa1fffff\"  # change the area to observe\n",
    "example_index = pivot_df.columns.get_loc(example_area)\n",
    "np.save('prognose_RF.npy', predictions[:, example_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
