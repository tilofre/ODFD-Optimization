{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import deque, defaultdict\n",
    "import joblib\n",
    "import h3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import q_utils.abmQ as abm3\n",
    "import q_utils.repositioning as repositioning2\n",
    "from q_utils.RLSystemMetrics import SystemMonitor\n",
    "from q_utils.Q_agent import QLearningAgent, EnhancedRewardCalculator, EnhancedStateHandler\n",
    "from q_utils.RLAgentVisualizer import RLAgentVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "data_4 = pd.read_csv(\"Data/TrainData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freud\\AppData\\Local\\Temp\\ipykernel_36100\\1963110665.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders['time_bin'] = orders['platform_order_date'].dt.floor('15min')\n"
     ]
    }
   ],
   "source": [
    "# Unix time for one week before test data as training data\n",
    "initial_timestart = 1666591200-86400*7\n",
    "orders = data_4\n",
    "start_time = pd.to_datetime(initial_timestart, unit='s') + pd.Timedelta(hours=8)\n",
    "\n",
    "# 3 hours\n",
    "end_time = start_time + pd.Timedelta(hours=3)\n",
    "\n",
    "# Filter for time\n",
    "orders['platform_order_date'] = pd.to_datetime(orders['platform_order_date'])\n",
    "orders = orders[\n",
    "    (orders['platform_order_date'] > start_time) &\n",
    "    (orders['platform_order_date'] < end_time)\n",
    "]\n",
    "\n",
    "# Create bins\n",
    "orders['time_bin'] = orders['platform_order_date'].dt.floor('15min')\n",
    "\n",
    "# Number of orders per bin\n",
    "actual_demand = (\n",
    "    orders.groupby(['time_bin', 'hex_id'])\n",
    "    .size()\n",
    "    .reset_index(name='actual_order_count')\n",
    ")\n",
    "\n",
    "# Same format as for prediction\n",
    "wide_actual = actual_demand.pivot(\n",
    "    index='time_bin',\n",
    "    columns='hex_id',\n",
    "    values='actual_order_count'\n",
    ").fillna(0)\n",
    "pre_binned_demand = wide_actual.to_dict(orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the state handler with reward calculator, Q-Agent and for visuals the system monitor and visualizer\n",
    "state_handler = EnhancedStateHandler(n_distance_bins=10, n_courier_bins=10)\n",
    "reward_calculator = EnhancedRewardCalculator(base_reward=1.0)\n",
    "agent = QLearningAgent(learning_rate=0.2, discount_factor=0.95, epsilon=1.0)\n",
    "monitor = SystemMonitor()\n",
    "visualizer = RLAgentVisualizer(agent, state_handler, grid=None, monitor=monitor)\n",
    "steps = 30\n",
    "#The constants as for the ABM\n",
    "constants = {\n",
    "    'initial_timestart': 1666591200-86400*7,\n",
    "    'SPEED_HEX_PER_STEP': 8,\n",
    "    'simulation_duration_hours': 3,\n",
    "    'steps': 30,\n",
    "    'repositioning_interval': 15 * 60,\n",
    "    'MAX_ACCEPTABLE_DELAY_SECONDS': 5 * 60,\n",
    "    'MAX_QUEUE_ATTEMPTS': 20,\n",
    "    'pre_binned_demand': pre_binned_demand,\n",
    "    'MACRO_RESOLUTION': 8,\n",
    "    'WORK_RESOLUTION': 13\n",
    "}\n",
    "timestart = constants['initial_timestart']\n",
    "timeend = timestart + 3600 * constants['simulation_duration_hours']\n",
    "NUM_EPISODES = 500 \n",
    "epsilon_decay = 0.995\n",
    "\n",
    "sim_data_master = data_4.copy() \n",
    "rejection_model = joblib.load('Data/rejection_model.joblib')\n",
    "\n",
    "timestart = constants['initial_timestart']\n",
    "last_order_time = sim_data_master['platform_order_time'].max()\n",
    "TIME_BUFFER_SECONDS = 15 * 60\n",
    "timeend = last_order_time + TIME_BUFFER_SECONDS\n",
    "\n",
    "episode_metrics = defaultdict(list)\n",
    "\n",
    "base_coverage = 0.8\n",
    "variation_range = 0.3\n",
    "target_utilization = 0.25\n",
    "\n",
    "WARMUP_DURATION_SECONDS = 30*60\n",
    "\n",
    "learning_start_time = timestart + WARMUP_DURATION_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starte Episode 1/1000 | Epsilon: 1.000 | Kuriere: 743 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     dynamic_demand \u001b[38;5;241m=\u001b[39m pre_binned_demand\u001b[38;5;241m.\u001b[39mget(first_bin_key, {})\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dynamic_demand:\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mrepositioning2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_repositioning_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcouriers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_demand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPEED_HEX_PER_STEP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMACRO_RESOLUTION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWORK_RESOLUTION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_time \u001b[38;5;241m<\u001b[39m timeend:\n\u001b[0;32m     36\u001b[0m     \n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# 1. SYSTEM-UPDATE\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     couriers, delivered_orders \u001b[38;5;241m=\u001b[39m abm3\u001b[38;5;241m.\u001b[39mupdate_couriers_and_system(\n\u001b[0;32m     39\u001b[0m         current_time, constants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m], couriers, delivered_orders, constants, pre_binned_demand, order_queue\n\u001b[0;32m     40\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\freud\\OneDrive - Universit채t M체nster\\Master Thesis\\Python Code Thesis\\q_utils\\repositioning.py:170\u001b[0m, in \u001b[0;36mrun_repositioning_strategy\u001b[1;34m(couriers, demand_map, timestart, order_queue, speed_hex_per_step, steps, macro_resolution, work_resolution)\u001b[0m\n\u001b[0;32m    167\u001b[0m surplus_zones, deficit_zones \u001b[38;5;241m=\u001b[39m identify_courier_demand_zones(surplus_deficit_map)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m surplus_zones \u001b[38;5;129;01mand\u001b[39;00m deficit_zones:\n\u001b[1;32m--> 170\u001b[0m     \u001b[43mexecute_repositioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43midle_couriers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurplus_zones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeficit_zones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed_hex_per_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmacro_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwork_resolution\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\freud\\OneDrive - Universit채t M체nster\\Master Thesis\\Python Code Thesis\\q_utils\\repositioning.py:122\u001b[0m, in \u001b[0;36mexecute_repositioning\u001b[1;34m(idle_couriers, surplus_zones, deficit_zones, timestart, order_queue, speed_hex_per_step, steps, macro_resolution, work_resolution)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# sometimes min ETA in best surplus zone is still to far away\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Here we want to travel only 50% of the distance if its in our max repo time\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actual_travel_time \u001b[38;5;241m>\u001b[39m max_repo_time_seconds: \u001b[38;5;66;03m#max allowed travel time\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m \u001b[43mh3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_path_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcourier_to_move\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_target_hex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_path) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    124\u001b[0m         midpoint_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_path) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\freud\\anaconda3\\envs\\tf310\\lib\\site-packages\\h3\\api\\basic_str\\__init__.py:901\u001b[0m, in \u001b[0;36mgrid_path_cells\u001b[1;34m(start, end)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03mReturns the ordered collection of cells denoting a\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03mminimum-length non-unique path between cells.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m    Starting with ``start``, and ending with ``end``.\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    899\u001b[0m mv \u001b[38;5;241m=\u001b[39m _cy\u001b[38;5;241m.\u001b[39mgrid_path_cells(_in_scalar(start), _in_scalar(end))\n\u001b[1;32m--> 901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_out_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\freud\\anaconda3\\envs\\tf310\\lib\\site-packages\\h3\\api\\basic_str\\_convert.py:14\u001b[0m, in \u001b[0;36m_out_collection\u001b[1;34m(mv)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_out_collection\u001b[39m(mv):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_cy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\freud\\anaconda3\\envs\\tf310\\lib\\site-packages\\h3\\api\\basic_str\\_convert.py:14\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_out_collection\u001b[39m(mv):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_cy\u001b[38;5;241m.\u001b[39mint_to_str(h) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m mv)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(NUM_EPISODES):\n",
    "    \n",
    "    # Logic for calculating dynamic courier numbers\n",
    "    avg_order_duration = 1800\n",
    "    orders_in_window = len(sim_data_master) / constants.get('simulation_duration_hours', 3)\n",
    "    required_courier_hours = (orders_in_window * avg_order_duration) / (3600*3)\n",
    "    coverage_adjustment = np.random.uniform(-variation_range, variation_range)\n",
    "    adjusted_coverage = max(0.3, min(0.9, base_coverage + coverage_adjustment))\n",
    "    num_couriers_for_episode = max(int(required_courier_hours / target_utilization * adjusted_coverage), 5)\n",
    "    \n",
    "    couriers = abm3.initiate_couriers(\n",
    "        total_couriers_to_create=num_couriers_for_episode,\n",
    "        data_source_df=sim_data_master\n",
    "    )    \n",
    "\n",
    "    # Initialising the episode\n",
    "    order_queue = []\n",
    "    episode_reward_sum = 0\n",
    "    decision_counter = 0\n",
    "    delivered_orders = set()\n",
    "    current_time = timestart\n",
    "    \n",
    "    print(f\"--- Starte Episode {episode + 1}/{NUM_EPISODES} | Epsilon: {agent.epsilon:.3f} | Kuriere: {len(couriers)} ---\")\n",
    "\n",
    "    # Pre warmup for courier repositioning \n",
    "    warmup_seconds = 15 * 60 \n",
    "    warmup_start_time_pre = constants['initial_timestart']  - warmup_seconds\n",
    "    for t in range(warmup_start_time_pre, constants['initial_timestart'] , constants['steps'] ):\n",
    "        couriers, _, _ = abm3.move_couriers_new(couriers, timestart, (0,0,0,0), delivered_orders, constants['SPEED_HEX_PER_STEP'], constants['steps'])\n",
    "        first_bin_key = pd.to_datetime(constants['initial_timestart'] , unit='s').floor('15min') + pd.Timedelta(hours=8)\n",
    "        dynamic_demand = pre_binned_demand.get(first_bin_key, {})\n",
    "        if dynamic_demand:\n",
    "            repositioning2.run_repositioning_strategy(couriers, dynamic_demand, t, [], constants['SPEED_HEX_PER_STEP'], constants['steps'], constants['MACRO_RESOLUTION'], constants['WORK_RESOLUTION'])\n",
    "\n",
    "    while current_time < timeend:\n",
    "        \n",
    "        # Update the system\n",
    "        couriers, delivered_orders = abm3.update_couriers_and_system(\n",
    "            current_time, constants['steps'], couriers, delivered_orders, constants, pre_binned_demand, order_queue\n",
    "        )\n",
    "\n",
    "        # Prepare orders\n",
    "        new_orders = abm3.get_new_orders(current_time, constants['steps'], sim_data_master)\n",
    "        orders_to_process = order_queue + [(order, 0) for order in new_orders]\n",
    "        next_order_queue = []\n",
    "\n",
    "        # Decide and learn\n",
    "        for order, attempts in orders_to_process:\n",
    "            # Needed to decide whether the job remains in the queue\n",
    "            success = False \n",
    "\n",
    "            # When an order waits too long\n",
    "            if attempts > constants.get('MAX_QUEUE_ATTEMPTS', 5):\n",
    "                #that the order is not too long in queue \n",
    "                processed_ids_set = set() \n",
    "                success, _ = abm3.handle_standard_assignment(\n",
    "                    order, attempts, couriers, current_time, constants, rejection_model, processed_ids_set\n",
    "                )\n",
    "                # No training because of a rule based decision\n",
    "\n",
    "            # Follow up of agent\n",
    "            else:\n",
    "\n",
    "            # 30 minutes warmup for courier\n",
    "                if current_time < learning_start_time:\n",
    "                    action = 0 #then always direct delivery\n",
    "                    state_features = state_handler.get_state_features(order, couriers)\n",
    "\n",
    "                    # Ignore the reward\n",
    "                    _reward, _done, success = abm3.execute_decision_for_order(\n",
    "                        order, action, couriers, current_time, constants, \n",
    "                        reward_calculator, state_handler, rejection_model, state_features\n",
    "                    )\n",
    "                    # Ignore training\n",
    "\n",
    "                # After 30 minutes\n",
    "                else:\n",
    "                    # get state features\n",
    "                    state_features = state_handler.get_state_features(order, couriers)\n",
    "                    state = state_handler.discretize_state(state_features)\n",
    "\n",
    "                    # get action of agent from table\n",
    "                    action = agent.get_action(state)\n",
    "\n",
    "                    # execute decision and receive reward\n",
    "                    reward, done, success = abm3.execute_decision_for_order(\n",
    "                        order, action, couriers, current_time, constants, \n",
    "                        reward_calculator, state_handler, rejection_model, state_features\n",
    "                    )\n",
    "\n",
    "                    # Get next state\n",
    "                    next_state_features = state_handler.get_state_features(order, couriers)\n",
    "                    next_state = state_handler.discretize_state(next_state_features)\n",
    "\n",
    "                    # Agent learns \n",
    "                    agent.learn(state, action, reward, next_state)\n",
    "\n",
    "                    # Update metrics\n",
    "                    episode_reward_sum += reward\n",
    "                    decision_counter += 1\n",
    "                    visualizer.record_decision(order, state_features, state, action, reward, couriers)\n",
    "\n",
    "            # Update Queue\n",
    "            if not success:\n",
    "                next_order_queue.append((order, attempts + 1))\n",
    "        \n",
    "        order_queue = next_order_queue\n",
    "        \n",
    "        #From old logic does nothing but get active couriers\n",
    "        active_couriers = [c for c in couriers if c.state != 'INACTIVE']\n",
    "        if active_couriers:\n",
    "            available_count = sum(1 for c in active_couriers if c.state == 'IDLE')\n",
    "            busy_count = len(active_couriers) - available_count\n",
    "            inactive_count = len(couriers) - len(active_couriers)\n",
    "            \n",
    "            # NOTE: The monitor was designed for a different loop structure.\n",
    "            # We are feeding it with aggregated data from this timestep for visualization purposes.\n",
    "            # Here, we collect the actions taken on NEW orders in this step.\n",
    "            decisions_in_step = []\n",
    "            for order, attempts in orders_to_process:\n",
    "                if attempts == 0: # This was a new order\n",
    "                    # We need to re-calculate the action for the monitor as it's not stored\n",
    "                    state_features = state_handler.get_state_features(order, couriers)\n",
    "                    state = state_handler.discretize_state(state_features)\n",
    "                    action = agent.get_action(state)\n",
    "                    decisions_in_step.append(action)\n",
    "\n",
    "            # Record the state of this timestep in the monitor\n",
    "            monitor.record_step(\n",
    "                available_count,\n",
    "                busy_count,\n",
    "                inactive_count,\n",
    "                new_orders, \n",
    "                decisions_in_step, \n",
    "                [episode_reward_sum / decision_counter if decision_counter > 0 else 0] \n",
    "            )\n",
    "            \n",
    "        current_time += constants['steps']\n",
    "\n",
    "\n",
    "    # End of episode\n",
    "    monitor.end_episode() #monitor episode\n",
    "    agent.decrease_epsilon(decay=epsilon_decay) #update epsilon\n",
    "    \n",
    "    avg_reward_per_decision = episode_reward_sum / decision_counter if decision_counter > 0 else 0 #calculate average reward per decision\n",
    "    episode_metrics['rewards'].append(avg_reward_per_decision) #append it for metrics in a list\n",
    "    \n",
    "    print(\n",
    "        f\"Episode {episode + 1} done. \"\n",
    "        f\"Decisions: {decision_counter} | \"\n",
    "        f\"Avg. Reward/Order: {avg_reward_per_decision:.3f}\"\n",
    "    )\n",
    "        \n",
    "    # --- VISUALISIERUNG ---\n",
    "    plot_dir = \"training_plots\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(\"\\n--- Evaluations-Checkpoint ---\")\n",
    "        try:\n",
    "            fig_policy = visualizer.visualize_policy()\n",
    "            policy_plot_filename = os.path.join(plot_dir, f\"policy_episode_{episode + 1}.svg\")\n",
    "            plt.savefig(policy_plot_filename, format='svg', bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            fig_per_decision = visualizer.visualize_learning_progress(reward_metric='per_decision')\n",
    "            learning_plot_filename = os.path.join(plot_dir, f\"learning_episode_{episode + 1}.svg\")\n",
    "            plt.savefig(learning_plot_filename, format='svg', bbox_inches='tight') \n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errroooorororo': {e}\")\n",
    "        finally:\n",
    "            print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(agent.q_table, 'q_learning_agent_nbins_00_1000.joblib')\n",
    "\n",
    "print(\"Training abgeschlossen und Q-Tabelle in 'q_learning_agent_final.joblib' gespeichert!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
